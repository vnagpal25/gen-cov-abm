{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31876d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /Users/vnagpal/Desktop/fa-2025/cse-598-ai4sci/gen-cov-abm\n"
     ]
    }
   ],
   "source": [
    "# allow proper pathing for project\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# get absolute path to project root\n",
    "ROOT = Path(__file__).resolve().parents[1] if \"__file__\" in locals() else Path.cwd().parents[0]\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "print(\"Added to sys.path:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a8ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ESM imported successfully\n"
     ]
    }
   ],
   "source": [
    "from src.utils.path_utils import get_data_dir\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b9852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ESM pretrained module available: True\n",
      "✓ ESM2 model available: True\n"
     ]
    }
   ],
   "source": [
    "# verify ESM installation\n",
    "print(\"ESM pretrained module available:\", hasattr(esm, 'pretrained'))\n",
    "print(\"ESM2 model available:\", hasattr(esm.pretrained, 'esm2_t33_650M_UR50D') if hasattr(esm, 'pretrained') else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d9cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "seq_file_path = get_data_dir() / \"ma_sequences.csv\"\n",
    "seq_df = pd.read_csv(seq_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9f147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ESM-2 model (this may take a while on first run)...\n",
      "NOTE: If you get an error about 'esm.pretrained', restart the kernel!\n",
      "✓ Model loaded successfully\n",
      "✓ Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "# Load pretrained ESM-2 model\n",
    "print(\"Loading ESM-2 model (this may take a while on first run)...\")\n",
    "print(\"NOTE: If you get an error about 'esm.pretrained', restart the kernel!\")\n",
    "\n",
    "# Load ESM-2 model using fair-esm 2.0+ API\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "model.eval()  # Disable dropout\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"✓ Model loaded successfully\")\n",
    "print(f\"✓ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1304f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Generating N-sequence embeddings...\n",
      "============================================================\n",
      "Processing 4070 sequences from 'n_sequence' column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [22:07<00:00, 10.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embeddings shape for 'n_sequence': (4070, 1280)\n",
      "\n",
      "============================================================\n",
      "Generating S-sequence embeddings...\n",
      "============================================================\n",
      "Processing 4070 sequences from 's_sequence' column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 7/128 [2:24:54<39:17:33, 1169.04s/it]"
     ]
    }
   ],
   "source": [
    "# Helper function to remove stop codons\n",
    "def remove_stop_codon(sequence):\n",
    "    if sequence[-1] == \"*\":\n",
    "        return sequence[:-1]\n",
    "    return sequence\n",
    "\n",
    "\n",
    "# Function to generate embeddings for a given sequence column\n",
    "def generate_embeddings(seq_df, seq_column, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate embeddings for sequences in a specified column.\n",
    "    \n",
    "    Args:\n",
    "        seq_df: DataFrame containing sequences\n",
    "        seq_column: Name of the column containing sequences\n",
    "        batch_size: Number of sequences to process per batch\n",
    "    \n",
    "    Returns:\n",
    "        embeddings_array: NumPy array of embeddings\n",
    "        ids: List of sequence IDs\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    all_ids = []\n",
    "    \n",
    "    # Clean sequences\n",
    "    seq_df_clean = seq_df.copy()\n",
    "    seq_df_clean[seq_column] = seq_df_clean[seq_column].apply(remove_stop_codon)\n",
    "    \n",
    "    print(f\"Processing {len(seq_df_clean)} sequences from '{seq_column}' column...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(seq_df_clean), batch_size)):\n",
    "        batch_df = seq_df_clean.iloc[i : i + batch_size]\n",
    "        \n",
    "        # Prepare batch data\n",
    "        data = [(row['name'], row[seq_column]) for _, row in batch_df.iterrows()]\n",
    "        \n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "            token_representations = results[\"representations\"][33]\n",
    "        \n",
    "        # Average across residues (excluding special tokens)\n",
    "        sequence_repr = token_representations.mean(1)\n",
    "        \n",
    "        # Store results\n",
    "        all_embeddings.append(sequence_repr.cpu().numpy())\n",
    "        all_ids.extend(batch_labels)\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    embeddings_array = np.vstack(all_embeddings)\n",
    "    print(f\"Final embeddings shape for '{seq_column}': {embeddings_array.shape}\")\n",
    "    \n",
    "    return embeddings_array, all_ids\n",
    "\n",
    "\n",
    "# Generate embeddings for both sequence types\n",
    "batch_size = 32  # Adjust based on GPU memory\n",
    "\n",
    "print(\"Generating N-sequence embeddings...\")\n",
    "n_embeddings, n_ids = generate_embeddings(seq_df, \"n_sequence\", batch_size)\n",
    "\n",
    "print(\"Generating S-sequence embeddings...\")\n",
    "s_embeddings, s_ids = generate_embeddings(seq_df, \"s_sequence\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify embeddings shapes\n",
    "print(f\"N-sequence embeddings shape: {n_embeddings.shape}\")\n",
    "print(f\"S-sequence embeddings shape: {s_embeddings.shape}\")\n",
    "print(f\"Number of sequences: {len(n_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to disk\n",
    "output_dir = get_data_dir()\n",
    "\n",
    "print(\"Saving N-sequence embeddings...\")\n",
    "# Save N-sequence embeddings as numpy array\n",
    "np.save(output_dir / \"n_sequence_embeddings.npy\", n_embeddings)\n",
    "print(f\"  ✓ Saved to {output_dir / 'n_sequence_embeddings.npy'}\")\n",
    "\n",
    "# Save N-sequence IDs\n",
    "with open(output_dir / \"n_sequence_ids.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(n_ids))\n",
    "print(f\"  ✓ Saved IDs to {output_dir / 'n_sequence_ids.txt'}\")\n",
    "\n",
    "# Save N-sequence as CSV\n",
    "n_embedding_df = pd.DataFrame(n_embeddings, index=n_ids)\n",
    "n_embedding_df.to_csv(output_dir / \"n_sequence_embeddings.csv\")\n",
    "print(f\"  ✓ Saved to {output_dir / 'n_sequence_embeddings.csv'}\")\n",
    "\n",
    "print(\"\\nSaving S-sequence embeddings...\")\n",
    "# Save S-sequence embeddings as numpy array\n",
    "np.save(output_dir / \"s_sequence_embeddings.npy\", s_embeddings)\n",
    "print(f\"  ✓ Saved to {output_dir / 's_sequence_embeddings.npy'}\")\n",
    "\n",
    "# Save S-sequence IDs\n",
    "with open(output_dir / \"s_sequence_ids.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(s_ids))\n",
    "print(f\"  ✓ Saved IDs to {output_dir / 's_sequence_ids.txt'}\")\n",
    "\n",
    "# Save S-sequence as CSV\n",
    "s_embedding_df = pd.DataFrame(s_embeddings, index=s_ids)\n",
    "s_embedding_df.to_csv(output_dir / \"s_sequence_embeddings.csv\")\n",
    "print(f\"  ✓ Saved to {output_dir / 's_sequence_embeddings.csv'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All embeddings saved successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTo load the embeddings later:\")\n",
    "print(\"  # N-sequence embeddings\")\n",
    "print(f\"  n_embeddings = np.load('{output_dir / 'n_sequence_embeddings.npy'}')\")\n",
    "print(f\"  with open('{output_dir / 'n_sequence_ids.txt'}') as f: n_ids = f.read().splitlines()\")\n",
    "print(\"\\n  # S-sequence embeddings\")\n",
    "print(f\"  s_embeddings = np.load('{output_dir / 's_sequence_embeddings.npy'}')\")\n",
    "print(f\"  with open('{output_dir / 's_sequence_ids.txt'}') as f: s_ids = f.read().splitlines()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load both sets of embeddings\n",
    "output_dir = get_data_dir()\n",
    "\n",
    "# Load N-sequence embeddings\n",
    "n_embeddings_loaded = np.load(output_dir / 'n_sequence_embeddings.npy')\n",
    "with open(output_dir / 'n_sequence_ids.txt') as f:\n",
    "    n_ids_loaded = f.read().splitlines()\n",
    "\n",
    "# Load S-sequence embeddings\n",
    "s_embeddings_loaded = np.load(output_dir / 's_sequence_embeddings.npy')\n",
    "with open(output_dir / 's_sequence_ids.txt') as f:\n",
    "    s_ids_loaded = f.read().splitlines()\n",
    "\n",
    "print(f\"N-sequence embeddings shape: {n_embeddings_loaded.shape}\")\n",
    "print(f\"S-sequence embeddings shape: {s_embeddings_loaded.shape}\")\n",
    "print(f\"Number of N-sequence IDs: {len(n_ids_loaded)}\")\n",
    "print(f\"Number of S-sequence IDs: {len(s_ids_loaded)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
